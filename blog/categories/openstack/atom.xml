<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Openstack | i4box's Blog]]></title>
  <link href="http://i4box.com/blog/categories/openstack/atom.xml" rel="self"/>
  <link href="http://i4box.com/"/>
  <updated>2017-12-13T13:48:46+08:00</updated>
  <id>http://i4box.com/</id>
  <author>
    <name><![CDATA[i4box]]></name>
    <email><![CDATA[jinhao2011@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sriov + Dpdk测试]]></title>
    <link href="http://i4box.com/blog/2017/12/13/sriov-plus-dpdkce-shi/"/>
    <updated>2017-12-13T13:00:25+08:00</updated>
    <id>http://i4box.com/blog/2017/12/13/sriov-plus-dpdkce-shi</id>
    <content type="html"><![CDATA[<h2>简介</h2>

<p>SRIOV是一种虚拟化硬件加速方案，支持SRIOV功能的网卡，可以从一个物理端口(PF),新虚拟出多个虚拟端口(VF),每个VF具有独享的PCI配置区域，与其他VF共享相同的物理资源或物理网口；
从逻辑上可以认为开启SR-IOV后的物理网卡内置了一个定制的Switch将所有的PF和VF口连接起来，通过VF和PF的mac地址以及vlanid来进行数据包分发。</p>

<!-- more -->


<ol>
<li>入方向（从外部进入网卡）上，数据包分发机制是：如果数据包的目的mac地址和vlanid都匹配某一个VF，那么数据包会分发到该VF，否则数据包会进入PF；如果数据包的目的mac地址是广播地址，那么数据包会在同一个vlan内广播（即所有vlanid=数据包vlanid的VF，都会收到该数据包）。</li>
<li>出方向（从PF或者VF发出）上：如果数据包的mac地址不匹配同一vlan内的任何端口（VF或PF），那么数据包会向网卡外部转发，否则会直接转发给对应的端口；如果数据包的mac地址为广播地址，那么数据包会在同一个vlan内以及向网卡外部广播。
<em>注：所有未设置vlanid的VF和PF，可以认为是在同一个“vlan”中，不带vlan的数据包在该“vlan”中按照上述规则进行处理</em>
<em>参考: <a href="http://www.jianshu.com/p/9bf690956d7d">http://www.jianshu.com/p/9bf690956d7d</a></em></li>
</ol>


<h2>开启SRIOV</h2>

<ol>
<li>确定网卡是否支持SRIOV特性，像Intel的X722、X710、82599都是支持的；</li>
<li>BIOS中开启SRIOV特性，大部分服务器默认开启；</li>
</ol>


<h2>配置SRIOV</h2>

<p>测试环境：CentOS7.3 内核：3.10  网卡：Intel X710</p>

<pre><code>开启两个PF(enp95s0f1是网卡名)
echo 2 &gt; /sys/class/net/enp95s0f1/device/sriov_numvfs

查看该网卡最多支持配置的VF个数
cat /sys/class/net/enp95s0f1/device/sriov_totalvfs
</code></pre>

<p>配置完成之后，查看分出来的VF网卡</p>

<pre><code>ip link show
</code></pre>

<p><img src="/images/2017/12/sriov-ip-link-show.png" alt="ip link show " /></p>

<h2>SRIOV + DPDK</h2>

<ol>
<li><p>将网卡绑定到dpdk igb_uio驱动</p>

<p> ./dpdk-devbind.py &ndash;bind=igb_uio 0000:5f:00.1</p></li>
<li><p>配置SRIOV，分出VF</p>

<p> echo 2 > /sys/bus/pci/devices/0000:5f:00.1/max_vfs</p></li>
<li><p>将网卡PF面挂到OVS</p>

<p> ovs-vsctl &ndash;timeout 10 add-port br-phy dpdk0 &ndash; set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:5f:00.1</p></li>
</ol>


<p>此时，OVS可以通过该网卡DPDK + PF面进行通信,分离出的VF网卡可以用于直接透传给虚机使用，或其他用途，如分给存储面用。</p>

<h2>网卡bond</h2>

<p>目标：将2张10G网卡通过SRIOV切分出PF面和VF面，PF面bond，通过dpdk挂载到ovs，做为数据面用，VF网卡用于虚机透传和存储。</p>

<ol>
<li><p>两张网卡都通过上面的步骤，挂到dpdk的igb_uio驱动，分出VF面；</p></li>
<li><p>将两个PF面bond到ovs上</p>

<p> ovs-vsctl add-bond br-phy dpdkbond p0 p1 &ndash; set Interface p0 type=dpdk options:dpdk-devargs=0000:5f:00.0 &ndash; set Interface p1 type=dpdk options:dpdk-devargs=0000:5f:00.1</p></li>
<li><p>设置bond模式</p>

<p> ovs-vsctl set port dpdkbond bond_mode=balance-slb</p></li>
</ol>


<h2>参考</h2>

<p><a href="http://blog.51cto.com/maomaostyle/1439651">http://blog.51cto.com/maomaostyle/1439651</a><br/>
<a href="http://www.jianshu.com/p/9bf690956d7d">http://www.jianshu.com/p/9bf690956d7d</a> <br/>
<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_openstack_platform/7/html/director_installation_and_usage/app-bonding_options">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_openstack_platform/7/html/director_installation_and_usage/app-bonding_options</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dpdk对接openstack]]></title>
    <link href="http://i4box.com/blog/2017/11/27/dpdkdui-jie-openstack/"/>
    <updated>2017-11-27T17:00:22+08:00</updated>
    <id>http://i4box.com/blog/2017/11/27/dpdkdui-jie-openstack</id>
    <content type="html"><![CDATA[<h2>安装</h2>

<h3>编译安装DPDK + OVS</h3>

<pre><code>参考 [DPDK安装详解与问题记录](http://www.i4box.com/blog/2017/08/21/dpdk-plus-ovsan-zhuang-xiang-jie/)
</code></pre>

<!-- more -->


<h3>启动OVS</h3>

<pre><code>1.  ./configure --with-dpdk=$DPDK_BUILD --prefix=/usr --localstatedir=/var --sysconfdir=/etc
2.  make &amp; make install
3.  export PATH=$PATH:/usr/share/openvswitch/scripts
    export DB_SOCK=/var/run/openvswitch/db.sock
4.  ovs-ctl --no-ovs-vswitchd start
5.  ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true
    ovs-ctl --no-ovsdb-server --db-sock="$DB_SOCK" start
</code></pre>

<p>到此，支持dpdk的ovs启动成功。</p>

<h3>创建相关网桥</h3>

<p>1、创建br-int网桥</p>

<pre><code>ovs-vsctl --may-exist add-br br-int -- set Bridge br-int datapath_type=netdev -- br-set-external-id br-int bridge-id br-int -- set bridge br-int fail-mode=standalone
</code></pre>

<p>2、创建br-phy网桥</p>

<pre><code>ovs-vsctl --may-exist add-br br-phy -- set Bridge br-phy datapath_type=netdev -- br-set-external-id br-phy bridge-id br-phy \
            -- set bridge br-phy fail-mode=standalone other_config:hwaddr=&lt;mac address of eth1 interface&gt;
</code></pre>

<p>说明：这个网桥是ovs运行在用户态下，多添加的，使ovs用户态能够使用linux内核态协议栈进行路由和arp解析。（原文如下）
<em>This additional bridge is required when running Open vSwitch in userspace rather than kernel-based Open vSwitch. The purpose of this bridge is to allow use of the kernel network stack for routing and ARP resolution. The datapath needs to look-up the routing table and ARP table to prepare the tunnel header and transmit data to the output port.</em></p>

<p>3、将eth1添加到br-phy上</p>

<p>这里eth1已结通过dpdk绑定igb_uio驱动</p>

<pre><code># ovs-vsctl --timeout 10 add-port br-phy dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:06:00.0
# ip addr add 172.168.1.1/24 dev br-phy
# ip link set br-phy up
# iptables -F
</code></pre>

<h3>配置openvswitch_agent</h3>

<p>ovs配置项下配置如下配置</p>

<pre><code>[ovs]
datapath_type=netdev
vhostuser_socket_dir=/usr/local/var/run/openvswitch
</code></pre>

<h3>设置nova flavor大页</h3>

<pre><code># sudo openstack flavor create --id 3 --ram 64 --vcpus 1 hugepage
# sudo openstack flavor set --property  hw:mem_page_size=large hugepage
</code></pre>

<h3>创建虚机，观察</h3>

<ol>
<li>dashboard上创建虚机，选择hugepage flavor；</li>
<li>创建成功后，ovs-vsctl show观察
<img src="/images/2017/12/ovs-dpdk-show.png" alt="vhostuser nic in ovs" /></li>
</ol>


<h2>问题记录</h2>

<p><strong>[1]</strong> libvirtError: internal error: process exited while connecting to monitor: 2017-11-20T12:18:04.693502Z qemu-kvm: -chardev socket,id=charnet0,path=/var/run/openvswitch/vhu360cc5c5-c9,server: Failed to bind socket to /var/run/openvswitch/vhu360cc5c5-c9: Permission denied</p>

<p>原因：由于我的Openstack环境是docker部署的，ovs被我移到宿主机上部署，docker中的nova没有权限操作宿主机的/var/run/openvswitch目录创建unix socket文件,
因此简单解决方法：chmod +777 /var/run/openvswitch</p>

<p><strong>[2]</strong> 两台compute节点上的虚机不通</p>

<p>排查方法: 检查配置在br-phy上的两个ip在宿主机上是否能够ping通。</p>

<h2>参考</h2>

<p><a href="http://docs.openvswitch.org/en/latest/intro/install/dpdk/">http://docs.openvswitch.org/en/latest/intro/install/dpdk/</a> <br/>
<a href="http://docs.openvswitch.org/en/latest/howto/dpdk/">http://docs.openvswitch.org/en/latest/howto/dpdk/</a><br/>
<a href="http://docs.openvswitch.org/en/latest/intro/install/general/#general-building">http://docs.openvswitch.org/en/latest/intro/install/general/#general-building</a><br/>
<a href="http://syswift.com/183.html">http://syswift.com/183.html</a><br/>
<a href="http://docs.openvswitch.org/en/latest/howto/userspace-tunneling/">http://docs.openvswitch.org/en/latest/howto/userspace-tunneling/</a></p>
]]></content>
  </entry>
  
</feed>
